# Core
fastapi>=0.109.0
a2wsgi>=1.9.0
python-dotenv>=1.0.0
pydantic>=2.5.3
uvicorn>=0.27.0
requests>=2.31.0

# AI & RAG
langchain>=0.1.0
langchain-groq>=0.0.1
langchain-community>=0.0.13
groq>=0.4.1

# LLM Infrastructure
litellm>=1.0.0  # Unified LLM interface with automatic provider fallback

# Reranking
cohere>=5.0.0  # Cohere Rerank API (free tier available)

# Hybrid Search
rank-bm25>=0.2.2  # Pure Python BM25 implementation

# Database
supabase>=2.3.0

# ML - API client
huggingface-hub>=0.20.0

# NOTE: Moved from heavy local models to API-based approach for serverless
# - Using LiteLLM for provider fallback (Groq → OpenAI → Claude)
# - Using Cohere API for reranking (free tier)
# - Using HuggingFace Inference API for embeddings
